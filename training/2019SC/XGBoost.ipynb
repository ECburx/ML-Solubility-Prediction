{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost\n",
    "\n",
    "One of the most promising and scalable end-to-end tree boosting systems is Extreme Gradient Boosting (XGBoost) introduced by Chen and Guestrin (2016), which employs a regularization technique termed ``gradient-based sampling'' to address overfitting and parallel processing to accelerate training. The scalability of XGBoost is attributed by its author to several vital systems and algorithmic optimizations, such as a new tree learning algorithm that handles sparse data and a theoretically justified weighted quantile sketch procedure that facilitates instance weight handling during approximate tree learning.\n",
    "\n",
    "Given a training set $\\{ x_i, y_i \\}^N_1$, Chen and Guestrin (2016)  define a tree ensemble model that uses $K$ additive functions as\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^K f_k(x_i),\\ f_k \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}$ is the space of regression trees (CART). Subsequently, the decision rules incorporated within the trees $q$ are utilized to classify the data into the corresponding leaves, and the final prediction is computed by summing the scores assigned to the relevant leaves $w$. To obtain the set of functions employed in the model, we aim to minimize the ensuing regularized objective function.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega (f_k)\n",
    "$$\n",
    "\n",
    "where $\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2$, $l$ is a differentiable convex loss function and $\\Omega$ penalizes the complexity of the regression tree functions. It is noteworthy that when the regularization parameter is adjusted to zero, the objective function reverts to the traditional gradient tree boosting method. Besides this regularized objective, XGBoost incorporates two additional techniques to further prevent over-fitting: Shrinkage introduced by Friedman (2002) and feature sub-sampling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data.dataset import Dataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from model.xgboost.XGBoost import XGBoost\n",
    "from model.abstractmodel import AbstractModel\n",
    "from ray import tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/TRN_All\")\n",
    "TST1 = Dataset.load_csv(\"ds/TST_1_All\")\n",
    "TST2 = Dataset.load_csv(\"ds/TST_2_All\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SPACE = {\n",
    "    \"n_estimators\": tune.choice([100, 150, 200, 250, 300, 400, 500, 700, 900]),\n",
    "    \"learning_rate\": tune.choice([0.1, 0.01, 0.001]),\n",
    "    \"gamma\": tune.uniform(0, 1),\n",
    "    \"subsample\": tune.uniform(0, 1),\n",
    "    \"reg_alpha\": tune.uniform(0, 1),\n",
    "    \"reg_lambda\": tune.uniform(0, 1),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainable_func(config: dict, dataset: Dataset):\n",
    "    trn, val = dataset.split()\n",
    "    model = XGBoost(\n",
    "        task_type=\"regression\",\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        reg_alpha=config[\"reg_alpha\"],\n",
    "        reg_lambda=config[\"reg_lambda\"],\n",
    "        verbose=False\n",
    "        # eval_metric=mean_squared_error,\n",
    "        # early_stopping_rounds=10\n",
    "    )\n",
    "    # model.fit(trn, verbose=True, eval_set=[(val.X, val.y)])\n",
    "    model.fit(trn)\n",
    "    pred = model.predict(val)\n",
    "    tune.report(rmse=mean_squared_error(val.y, pred, squared=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.tune.search import BasicVariantGenerator\n",
    "\n",
    "tuner = AbstractModel.tuner(\n",
    "    trainable_func,\n",
    "    SPACE,\n",
    "    num_samples=100,\n",
    "    search_alg=BasicVariantGenerator(max_concurrent=1),\n",
    "    dataset=TRN,\n",
    "    metric_columns=[\"rmse\"]\n",
    ")\n",
    "tune_result = tuner.fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_result = tune_result.get_best_result(metric=\"rmse\", mode=\"min\")\n",
    "best_result.config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/all/TRN_All\")\n",
    "TST1 = Dataset.load_csv(\"ds/all/TST_1_All\")\n",
    "TST2 = Dataset.load_csv(\"ds/all/TST_2_All\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/2019trn/TRN_All\")\n",
    "TST1 = Dataset.load_csv(\"ds/all/TST_1_All\")\n",
    "TST2 = Dataset.load_csv(\"ds/all/TST_2_All\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trn_sets, val_sets = TRN.k_fold_split(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_pred_tst1 = []\n",
    "k_pred_tst2 = []\n",
    "\n",
    "for trn, val in zip(trn_sets, val_sets):\n",
    "    model = XGBoost(\n",
    "        task_type=\"regression\",\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=175,\n",
    "        subsample=0.4,\n",
    "        # reg_alpha=0.,\n",
    "        # reg_lambda=0.4,\n",
    "        # gamma=0.3,\n",
    "        # grow_policy=\"lossguide\",\n",
    "        # random_state=1234\n",
    "    )\n",
    "    model.fit(trn)\n",
    "    k_pred_tst1.append(model.predict(TST1))\n",
    "    k_pred_tst2.append(model.predict(TST2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pred_tst1 = pd.concat([pd.DataFrame(k_pred) for k_pred in k_pred_tst1], axis=1).mean(axis=1)\n",
    "pred_tst2 = pd.concat([pd.DataFrame(k_pred) for k_pred in k_pred_tst2], axis=1).mean(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"TST1 : RMSE {mean_squared_error(TST1.y, pred_tst1, squared=False)}\")\n",
    "print(f\"TST2 : RMSE {mean_squared_error(TST2.y, pred_tst2, squared=False)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"TST1 : R^2 {r2_score(TST1.y, pred_tst1)}\")\n",
    "print(f\"TST2 : R^2 {r2_score(TST2.y, pred_tst2)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"XGBoost\"\n",
    "min_ax1, max_ax1 = -7, -1\n",
    "min_ax2, max_ax2 = -11, -1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "sns.lineplot(x=[min_ax1, max_ax1], y=[min_ax1, max_ax1], ax=ax1, color=\"black\")\n",
    "sns.lineplot(x=[min_ax2, max_ax2], y=[min_ax2, max_ax2], ax=ax2, color=\"black\")\n",
    "\n",
    "sns.regplot(\n",
    "    data=pd.DataFrame({\n",
    "        \"True logS (mol/L)\": TST1.y.values[:, 0],\n",
    "        \"Predicted logS (mol/L)\": pred_tst1\n",
    "    }),\n",
    "    x=\"True logS (mol/L)\",\n",
    "    y=\"Predicted logS (mol/L)\",\n",
    "    ax=ax1\n",
    ")\n",
    "sns.regplot(\n",
    "    data=pd.DataFrame({\n",
    "        \"True logS (mol/L)\": TST2.y.values[:, 0],\n",
    "        \"Predicted logS (mol/L)\": pred_tst2\n",
    "    }),\n",
    "    x=\"True logS (mol/L)\",\n",
    "    y=\"Predicted logS (mol/L)\",\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "ax1.set_xlim(min_ax1, max_ax1)\n",
    "ax1.set_ylim(min_ax1, max_ax1)\n",
    "ax2.set_xlim(min_ax2, max_ax2)\n",
    "ax2.set_ylim(min_ax2, max_ax2)\n",
    "\n",
    "ax1.set_title(f\"2019 Solubility Challenge Test Set 1 ({model_name})\\n\"\n",
    "              f\"RMSE: {mean_squared_error(TST1.y, pred_tst1, squared=False):.3f}, $R^2$: {r2_score(TST1.y, pred_tst1):.3f}\")\n",
    "ax2.set_title(f\"2019 Solubility Challenge Test Set 2 ({model_name})\\n\"\n",
    "              f\"RMSE: {mean_squared_error(TST2.y, pred_tst2, squared=False):.3f}, $R^2$: {r2_score(TST2.y, pred_tst2):.3f}\")\n",
    "\n",
    "# plt.axis(\"equal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/all/UMAP_100d_TRN\")\n",
    "TST1 = Dataset.load_csv(\"ds/all/UMAP_100d_TST1\")\n",
    "TST2 = Dataset.load_csv(\"ds/all/UMAP_100d_TST2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = XGBoost(\n",
    "    task_type=\"regression\",\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=600,\n",
    "    subsample=0.3,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.1,\n",
    "    gamma=0.1,\n",
    ")\n",
    "model.fit(TRN, verbose=True, eval_set=[(TST1.X, TST1.y)])\n",
    "print(r2_score(TST1.y, model.predict(TST1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(TRN, verbose=True, eval_set=[(TST1.X, TST1.y)])\n",
    "print(r2_score(TST1.y, model.predict(TST1)))\n",
    "model.fit(TRN, verbose=True, eval_set=[(TST2.X, TST2.y)])\n",
    "print(r2_score(TST2.y, model.predict(TST2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
