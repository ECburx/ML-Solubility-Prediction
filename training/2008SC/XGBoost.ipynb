{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost\n",
    "\n",
    "One of the most promising and scalable end-to-end tree boosting systems is Extreme Gradient Boosting (XGBoost) introduced by Chen and Guestrin (2016), which employs a regularization technique termed ``gradient-based sampling'' to address overfitting and parallel processing to accelerate training. The scalability of XGBoost is attributed by its author to several vital systems and algorithmic optimizations, such as a new tree learning algorithm that handles sparse data and a theoretically justified weighted quantile sketch procedure that facilitates instance weight handling during approximate tree learning.\n",
    "\n",
    "Given a training set $\\{ x_i, y_i \\}^N_1$, Chen and Guestrin (2016)  define a tree ensemble model that uses $K$ additive functions as\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^K f_k(x_i),\\ f_k \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}$ is the space of regression trees (CART). Subsequently, the decision rules incorporated within the trees $q$ are utilized to classify the data into the corresponding leaves, and the final prediction is computed by summing the scores assigned to the relevant leaves $w$. To obtain the set of functions employed in the model, we aim to minimize the ensuing regularized objective function.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega (f_k)\n",
    "$$\n",
    "\n",
    "where $\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2$, $l$ is a differentiable convex loss function and $\\Omega$ penalizes the complexity of the regression tree functions. It is noteworthy that when the regularization parameter is adjusted to zero, the objective function reverts to the traditional gradient tree boosting method. Besides this regularized objective, XGBoost incorporates two additional techniques to further prevent over-fitting: Shrinkage introduced by Friedman (2002) and feature sub-sampling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from data.dataset import Dataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from model.xgboost.XGBoost import XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/UMAP_100d_TRN\")\n",
    "TST = Dataset.load_csv(\"ds/UMAP_100d_TST\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "TRN = Dataset.load_csv(\"ds/08SC/TRN_All\")\n",
    "TST = Dataset.load_csv(\"ds/TST_All\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = XGBoost(\n",
    "    task_type=\"regression\",\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=800,\n",
    "    subsample=0.2,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    gamma=0.1,\n",
    ")\n",
    "model.fit(TRN)\n",
    "# model.fit(TRN, verbose=True, eval_set=[(TST.X, TST.y)])\n",
    "print(r2_score(TST.y, model.predict(TST)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
